# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wLp16PkAwHKOzeziH8lI9UuO0Om2JvwW
"""

import numpy as np
import random

# Define a small 5x5 maze. 1 = wall, 0 = open path. Exit at (4,4)
maze = np.array([
    [0, 1, 0, 0, 0],
    [0, 1, 0, 1, 0],
    [0, 0, 0, 1, 0],
    [1, 0, 1, 0, 0],
    [0, 0, 0, 0, 0]
])
exit_pos = (4, 4)

# Directions: 0=up (-y'), 1=down (+y), 2=left (-x'), 3=right (+x)
directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]
dir_labels = ["y'", 'y', "x'", 'x']

class Agent:
    def __init__(self, pos):
        self.pos = pos

class XAgent(Agent):
    def measure(self, maze):
        x, y = self.pos  # (row, col)
        binaries = []
        for i, (dx, dy) in enumerate(directions):
            nx, ny = x + dx, y + dy
            if 0 <= nx < maze.shape[0] and 0 <= ny < maze.shape[1]:
                binaries.append(1 if maze[nx, ny] == 1 else 0)  # 1=edge (wall), 0=face (open)
            else:
                binaries.append(1)  # Boundary is wall
        return {'pos': (y, x), 'dirs': dict(zip(dir_labels, binaries))}  # pos as (col, row) for (x,y)

    def move(self, maze, o_pos, flee_dist=2):
        dist = np.linalg.norm(np.array(self.pos) - np.array(o_pos))
        possible_moves = []
        for d in directions:
            nx, ny = self.pos[0] + d[0], self.pos[1] + d[1]
            if 0 <= nx < maze.shape[0] and 0 <= ny < maze.shape[1] and maze[nx, ny] == 0:
                possible_moves.append(d)
        if possible_moves:
            if dist < flee_dist:
                # Flee: prefer away from o
                dx, dy = np.array(o_pos) - np.array(self.pos)
                away_dirs = [d for d in possible_moves if np.dot(d, (dy, dx)) < 0]  # Approximate opposite
                if away_dirs:
                    possible_moves = away_dirs
            self.pos = tuple(np.array(self.pos) + random.choice(possible_moves))

class CircleAgent(Agent):
    def __init__(self, pos):
        self.pos = pos
        self.knowledge = {}  # pos -> dirs

    def receive_info(self, info):
        self.knowledge[info['pos']] = info['dirs']

    def cohesion_D(self, xs_positions, max_dist=4.0):
        if not xs_positions:
            return 0.1  # Minimum cohesion if no X's
        distances = [np.linalg.norm(np.array(self.pos) - np.array(x_pos)) for x_pos in xs_positions]
        avg_dist = np.mean(distances)
        # Inverse distance-based cohesion, capped between 0.1 and 1
        d = max(0.1, min(1.0, 1.0 / (avg_dist / max_dist)))
        return d

    def calculate_angle(self, xs_positions, target):
        dx_exit, dy_exit = np.array(target) - np.array(self.pos)
        base_angle = np.arctan2(dy_exit, dx_exit)

        # Geometric influence from X's
        if xs_positions:
            repulsion_vec = np.zeros(2)
            for x_pos in xs_positions:
                dx, dy = np.array(self.pos) - np.array(x_pos)
                dist = np.linalg.norm(dx, dy)
                if dist > 0:  # Avoid division by zero
                    repulsion_vec += (dx / dist, dy / dist) / dist  # Normalized and weighted by inverse distance
            repulsion_angle = np.arctan2(repulsion_vec[1], repulsion_vec[0]) if np.linalg.norm(repulsion_vec) > 0 else 0
        else:
            repulsion_angle = 0

        # Combine with cohesion weight
        d = self.cohesion_D(xs_positions)
        final_angle = base_angle * (1 - d) + repulsion_angle * d  # Blend toward exit or away from X's
        return final_angle  # For potential continuous use

class CircleAgentQL(CircleAgent):
    def move_ql(self, maze, Q, epsilon, xs, xs_positions):
        state = self.pos
        close_penalty = sum(-1 for x_pos in xs_positions if np.linalg.norm(np.array(state) - np.array(x_pos)) < 2)
        action = random.randint(0, 3) if np.random.rand() < epsilon else np.argmax(Q[state[0], state[1]])
        dx, dy = directions[action]
        next_pos = (state[0] + dx, state[1] + dy)
        next_knowledge_pos = (next_pos[1], next_pos[0])  # (x,y)
        if next_knowledge_pos in self.knowledge:
            dir_key = dir_labels[action]
            if self.knowledge[next_knowledge_pos][dir_key] == 1:  # Known wall in that direction
                action = random.randint(0, 3)  # Explore alternative
        dx, dy = directions[action]
        next_pos = (state[0] + dx, state[1] + dy)
        reward = -0.1 + close_penalty
        if not (0 <= next_pos[0] < maze.shape[0] and 0 <= next_pos[1] < maze.shape[1]):
            reward -= 1
            next_state = state
        elif maze[next_pos[0], next_pos[1]] == 1:
            reward -= 1
            next_state = state
        else:
            next_state = next_pos
            if next_state == exit_pos:
                reward = 10
        # Update Q
        best_next = np.max(Q[next_state[0], next_state[1]]) if next_state != state else 0
        Q[state[0], state[1], action] += 0.1 * (reward + 0.9 * best_next - Q[state[0], state[1], action])
        self.pos = next_state
        return reward, action

# Simulation (simplified for brevity)
def train():
    Q = np.zeros((5, 5, 4))
    for episode in range(100):
        o = CircleAgentQL((random.randint(0, 4), random.randint(0, 4)))
        while maze[o.pos[0], o.pos[1]] == 1:
            o.pos = (random.randint(0, 4), random.randint(0, 4))
        xs = [XAgent((random.randint(0, 4), random.randint(0, 4))) for _ in range(4)]
        for x in xs:
            while maze[x.pos[0], x.pos[1]] == 1:
                x.pos = (random.randint(0, 4), random.randint(0, 4))
        xs_positions = [x.pos for x in xs]
        for t in range(50):
            for x in xs:
                x.move(maze, o.pos)
                o.receive_info(x.measure(maze))
            xs_positions = [x.pos for x in xs]
            reward, _ = o.move_ql(maze, Q, 0.1, xs, xs_positions)
            if o.pos == exit_pos:
                break
    return Q

def test(Q):
    o = CircleAgentQL((random.randint(0, 4), random.randint(0, 4)))
    while maze[o.pos[0], o.pos[1]] == 1:
        o.pos = (random.randint(0, 4), random.randint(0, 4))
    xs = [XAgent((random.randint(0, 4), random.randint(0, 4))) for _ in range(4)]
    for x in xs:
        while maze[x.pos[0], x.pos[1]] == 1:
            x.pos = (random.randint(0, 4), random.randint(0, 4))
    xs_positions = [x.pos for x in xs]
    steps = 0
    while steps < 50 and o.pos != exit_pos:
        for x in xs:
            x.move(maze, o.pos)
            o.receive_info(x.measure(maze))
        xs_positions = [x.pos for x in xs]
        _, _ = o.move_ql(maze, Q, 0.0, xs, xs_positions)  # Greedy
        steps += 1
    return o.pos == exit_pos

# Run
Q = train()
successes = sum(test(Q) for _ in range(10))
print(f"Success rate over 10 tests: {successes/10:.2f}")